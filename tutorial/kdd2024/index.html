<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multimodal Pretraining, Adaptation, and Generation for Recommender Systems</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
<header>
    <div class="cover-image">
        <div class="tutorial-info">
            <h1>Multimodal Pretraining, Adaptation, and Generation for Recommendation</h1>
            <p>In conjunction with <strong>KDD 2024</strong></p>
            <p>Time: Aug. 25, 2024 (10:00 AM - 1:00 PM)</p>
            <p>Location: Centre de Convencions Internacional de Barcelona</p>
        </div>
    </div>
</header>

<nav>
    <ul>
        <li><a href="#home">Home</a></li>
        <li><a href="#program">Program</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
        <li><a href="#previous">Previous Tutorials</a></li>
    </ul>
</nav>

<main>
    <section id="home">
        <h2>KDD 2024 Tutorial on "Multimodal Pretraining, Adaptation, and Generation for Recommendation"</h2>
        <p>&nbsp;</p>
        <p>Personalized recommendation serves as a ubiquitous channel for users to discover information or items tailored to their interests. However, prevalent recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in pretrained language/multimodal models offer new opportunities and challenges in developing content-aware recommender systems.</p>
        <p>&nbsp;</p>
        <p>This tutorial seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications to recommender systems. The tutorial is divided into five parts, covering multimodal pretraining, multimodal adaptation, multimodal generation, applications, and open challenges in the field of recommendation. By providing a succinct overview of the field, we aspire to facilitate a swift understanding of multimodal recommendation and promote meaningful discussions on the future development of this evolving landscape.</p>
        <p>&nbsp;</p>
        <p>Our tutorial will be held on Aug. 25, 2024, from 10:00 AM to 1:00 PM at the Centre de Convencions Internacional de Barcelona, in conjunction with KDD 2024. We welcome researchers, practitioners, and students interested in multimodal recommendation to join us and engage in this exciting tutorial.</p>
    </section>

    <section id="program">
<!--        <h2>Program</h2> add barcelona timezone-->
        <h2>Program (UTC+2, Barcelona Time)</h2>
        <p>&nbsp;</p>
        <p>You are welcome to join our tutorial either in-person or virtually via Zoom. The Zoom link will be provided closer to the tutorial date. The program schedule is as follows:</p>
        <table>
            <thead>
            <tr>
                <th>Time</th>
                <th>Event</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td>9:00 AM - 10:00 AM</td>
                <td>Opening Remarks & Overview</td>
            </tr>
            <!-- Add more rows for each program item -->
            </tbody>
        </table>
    </section>

    <section id="organizers">
        <h2>Organizers</h2>
        <p>&nbsp;</p>
        <div class="organizer-box">
            <div class="organizer">
                <img src="https://jiemingzhu.github.io/img/jmzhu.jpg" alt="Jieming ZHU">
                <p><strong>Jieming ZHU</strong></p>
                <p>Huawei Noah's Ark Lab</p>
            </div>
            <div class="organizer">
                <img src="https://liu.qijiong.work/images/resume-avatar.jpg" alt="Qijiong LIU">
                <p><strong>Qijiong LIU</strong></p>
                <p>The HK PolyU</p>
            </div>
            <div class="organizer">
                <img src="organizers/quanyu.png" alt="Quanyu DAI" style="object-position: 0 -25px">
                <p><strong>Quanyu DAI</strong></p>
                <p>Huawei Noah's Ark Lab</p>
            </div>
            <div class="organizer">
                <img src="http://www4.comp.polyu.edu.hk/~csxmwu/images/lead-bg.jpg" alt="Xiao-Ming WU" style="object-position: 0 0">
                <p><strong>Xiao-Ming WU</strong></p>
                <p>The HK PolyU</p>
            </div>
            <div class="organizer">
                <img src="https://www.ruizhang.info/rui_1.jpg" alt="Rui ZHANG" style="object-position: 0 0">
                <p><strong>Rui ZHANG</strong></p>
                <p>HUST</p>
            </div>

        </div>
    </section>

    <section id="contact">
        <h2>Contact</h2>
        <p>Please contact <a href="https://jiemingzhu.github.io/">Jieming ZHU</a> for general inquiries.</p>
    </section>

    <section id="previous">
        <h2>Previous Tutorials</h2>
        <p><a href="https://mmrec.github.io/tutorial/www2024/">Multimodal Pretraining and Generation for Recommendation</a>, in conjunction with <strong>WWW 2024</strong></p>
    </section>
</main>
</body>
</html>
